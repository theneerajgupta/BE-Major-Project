{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf2a1c73",
   "metadata": {},
   "source": [
    "# File 03: Pre Processing Timeline Tweets\n",
    "\n",
    "In this file we are supposed to filter out all the tweets that are not in english. Fortunately or Unfortunately, while fetching tweets in the last file, we collected around 1.4 million tweets. Firstly we remove all the repeated tweets. This brings the number down to around 1.17 million. 1.17 million tweets is still a lot.\n",
    "\n",
    "Looking at the task manager we notice that we are not utilizing all the resources available on our machine. Out of 24 available threads on our main machine, only 4 were being utilized. To take advantage of rest of the threads, we would make use of 'multiprocessing' functions available in python. Python's 'Concurrent Futures' library has greatly simplified the process of multiprocessing and multithreading any function. Just by providing a function and a list of inputs, it can automagically handle all the thread management on its own.\n",
    "\n",
    "There was one limitation of technique though. Multiprocessing did not allowed us to return an array of indexes for the tweets that were in 'english'. To mitigate this issue we decided to save all the indexes in text files and later load them and stitch together another dataframe containing only enlgish tweets. This datafram was later stored as '03-user-tweets-english-only.csv'\n",
    "\n",
    "\n",
    "\n",
    "### Input File:\n",
    "- 02-user-tweets.csv ------------------> Contains: Usernames, Tweets(From Timeline)\n",
    "\n",
    "### Output File:\n",
    "- 03-user-tweets-english-only.csv -----> Contains: Username, Tweets(From Timeline, only English)\n",
    "\n",
    "### Steps:\n",
    "1. Load '02-user-tweets.csv' dataframe\n",
    "2. Make a function that would create pairs of start and end point, dividing the dataframe into smaller chunks\n",
    "3. Create a function 'process' that:\n",
    "    1. Loads tweets from the dataframe within a certain provided range\n",
    "    2. Checks the Tweets for their language\n",
    "    3. Saves only the indexes of tweets which are in english\n",
    "    4. Save the array as a '.txt' file in 'database' directory\n",
    "4. User Multiprocessing to process all the tweets in the dataframe using 'process' function\n",
    "5. Load all files from 'database' directory\n",
    "6. Read indexes of all the english tweets from the saved files\n",
    "7. Save all of these 'tweets + username' pair in another dataframe\n",
    "8. Save the new dataframe as '03-user-tweets-english-only.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0337bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all required libraries\n",
    "\n",
    "import concurrent.futures\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import langid\n",
    "from nltk.classify.textcat import TextCat\n",
    "from langdetect import detect\n",
    "import pickle\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fc5216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load '02-user-tweets.csv' dataframe\n",
    "\n",
    "df = pd.read_csv(\"db/02-user-tweets.csv\")\n",
    "df = df.drop_duplicates()\n",
    "df1 = pd.DataFrame(columns=['user', 'tweet'])\n",
    "database = df.values.tolist()\n",
    "output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24c181ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1171489"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2380d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to spilt data into small chunks\n",
    "def make_pairs(end=1400000, divs=100000) :\n",
    "    output = []\n",
    "    var = 0\n",
    "    while (var < end) :\n",
    "        lst = [var, var + divs]\n",
    "        output.append(lst)\n",
    "        var = var + divs\n",
    "\n",
    "    if (output[-1][1] > end) :\n",
    "        remove = output.pop()\n",
    "        output[-1][1] = end\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e83948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 15000], [15000, 30000], [30000, 45000], [45000, 60000], [60000, 75000], [75000, 90000], [90000, 105000], [105000, 120000], [120000, 135000], [135000, 150000], [150000, 165000], [165000, 180000], [180000, 195000], [195000, 210000], [210000, 225000], [225000, 240000], [240000, 255000], [255000, 270000], [270000, 285000], [285000, 300000], [300000, 315000], [315000, 330000], [330000, 345000], [345000, 360000], [360000, 375000], [375000, 390000], [390000, 405000], [405000, 420000], [420000, 435000], [435000, 450000], [450000, 465000], [465000, 480000], [480000, 495000], [495000, 510000], [510000, 525000], [525000, 540000], [540000, 555000], [555000, 570000], [570000, 585000], [585000, 600000], [600000, 615000], [615000, 630000], [630000, 645000], [645000, 660000], [660000, 675000], [675000, 690000], [690000, 705000], [705000, 720000], [720000, 735000], [735000, 750000], [750000, 765000], [765000, 780000], [780000, 795000], [795000, 810000], [810000, 825000], [825000, 840000], [840000, 855000], [855000, 870000], [870000, 885000], [885000, 900000], [900000, 915000], [915000, 930000], [930000, 945000], [945000, 960000], [960000, 975000], [975000, 990000], [990000, 1005000], [1005000, 1020000], [1020000, 1035000], [1035000, 1050000], [1050000, 1065000], [1065000, 1080000], [1080000, 1095000], [1095000, 1110000], [1110000, 1125000], [1125000, 1140000], [1140000, 1155000], [1155000, 1171489]]\n"
     ]
    }
   ],
   "source": [
    "# divide entire 'database' into chunks of size 15000\n",
    "div_size = 15000\n",
    "PAIRS = make_pairs(len(database), div_size)\n",
    "print(PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to filter out all tweets which are not in english\n",
    "def process(endpoints):\n",
    "    id = 0\n",
    "    value = endpoints[0]\n",
    "    \n",
    "    while (value > 0) :\n",
    "        value = value - div_size\n",
    "        id = id + 1\n",
    "\n",
    "#     yes = 0\n",
    "#     no = 0\n",
    "    array = []\n",
    "    for index in range(endpoints[0], endpoints[1]) :\n",
    "            try :\n",
    "                # database[index][1] = preprocess_text(database[index][1])\n",
    "                if detect(database[index][1]) == 'en' :\n",
    "                    yes = yes + 1\n",
    "                    array.append(index)\n",
    "                else :\n",
    "                    no = no + 1\n",
    "            except :\n",
    "                no = no + 1\n",
    "\n",
    "    # print(f\"{id} Processing Done : {yes} Tweets Detected\")\n",
    "    with open(f'dataframe/{id}.txt', 'wb') as fo:\n",
    "        pickle.dump(array, fo)\n",
    "        print(f\"{id}.txt Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion that would use multiprocessing...\n",
    "def main():\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        executor.map(process, PAIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44171bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read all saved file and combine into one single datafram\n",
    "def not_main() :\n",
    "    files = glob.glob('dataframe/*.txt')\n",
    "    index = []\n",
    "    for file in files:\n",
    "        with open(file, 'rb') as fo:\n",
    "            obj = pickle.load(fo)\n",
    "            for row in obj :\n",
    "                index.append(row)\n",
    "    index = sorted(index)\n",
    "    final = []\n",
    "    for num in tqdm(index):\n",
    "        final.append(database[num])\n",
    "    final_db = pd.DataFrame(final, columns=['USER', 'TWEET'])\n",
    "    final_db.to_csv('db/03-user-tweets-english-only.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35da3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# executing the program\n",
    "main()\n",
    "not_main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Major Project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
