{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559021ff",
   "metadata": {},
   "source": [
    "# File 04: Preprocessing and Splitting Original Dataset\n",
    "\n",
    "This file is quiet essential. This makes sure we have the necessary data to train our model. Our source that set contains 1.6 million labelled tweets. Out of which we have removed 20,000 tweets in 'File 01'. So to avoid training the model on data we might predict on in the future steps, we divide the entire dataset into 'main-data' and 'train-data'.\n",
    "\n",
    "We also preprocess all the tweets in this dataset.\n",
    "The preprocessing involves:\n",
    "- Removing @_usernames_\n",
    "- Removing Hashtags\n",
    "- Removing Hyperlinks\n",
    "- Removing extra spaces\n",
    "- Removing Any digits\n",
    "- Removing Stopwords\n",
    "- Removing Single Characters\n",
    "\n",
    "### Input File:\n",
    "- 1600k-noemoticons.csv\n",
    "\n",
    "### Ouptut File:\n",
    "- 04-main-data.csv -----> 20,000 Entries (10,000 Pos, 10,000 Neg)\n",
    "- 04-train-data.csv ----> 1,580,000 Entries (740,000 Pos, 740,000 Neg)\n",
    "\n",
    "### Steps:\n",
    "1. load required libraries (standard and machine leanring)\n",
    "2. load and format the dataset\n",
    "3. create functions that will preprocess the dataset\n",
    "4. apply preprocessing on all tweets\n",
    "5. convert the results into a dataframe\n",
    "6. split the dataframe into 'main' and 'train'\n",
    "7. save 'main' and 'train' dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63784ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all standard libraries\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfaaf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading '1600k-noemoticon.csv' dataset\n",
    "df = pd.read_csv(\"../db/1600k-noemoticon.csv\", header=None)\n",
    "df.isnull().values.any()\n",
    "df.rename(\n",
    "    columns = {\n",
    "    0: 'SENTIMENT',\n",
    "    1: 'ID',\n",
    "    2: 'DATE',\n",
    "    3: 'QUERY',\n",
    "    4: 'USERNAME',\n",
    "    5: 'TWEET'\n",
    "    }, inplace=True, errors='raise'\n",
    ")\n",
    "df = df[['USERNAME', 'SENTIMENT', 'TWEET']]\n",
    "df['TEXT'] = \"\"\n",
    "dataset = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d7d0fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TWEET'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45e6c75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# define functions preprocess the dataset...\n"
     ]
    }
   ],
   "source": [
    "# create functions that will preprocess the dataset\n",
    "porter = PorterStemmer()\n",
    "sw = stopwords.words('english')\n",
    "sw.remove('not')\n",
    "\n",
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def remove_single_chars(text) :\n",
    "    array = text.split()\n",
    "    return (\" \".join([w for w in array if len(w) > 1]))\n",
    "\n",
    "def remove_stopwords(text) :\n",
    "    text = \" \".join([word for word in text.split() if word not in sw])\n",
    "    return text\n",
    "\n",
    "def preprocess_text(sen) :\n",
    "    sentence = remove_tags(sen)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub('@[A-Za-z]+[A-Za-z0-9-_]+', '', sentence)\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    sentence = remove_single_chars(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d9fb137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1600000/1600000 [00:29<00:00, 53488.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply preprocessing on all tweets\n",
    "for node in tqdm(dataset):\n",
    "    if node[1] > 1 :\n",
    "        node[1] = 1\n",
    "    node[3] = preprocess_text(node[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3457408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "df = pd.DataFrame(dataset, columns=['USER', 'SENTIMENT', 'ORIGINAL', 'TEXT'])\n",
    "# df = df[['USER', 'TEXT', 'SENTIMENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a24bfe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe\n",
    "df_list = df.values.tolist()\n",
    "df_main = df_list[:10000] + df_list[800000:810000]\n",
    "df_train = df_list[10000:800000] + df_list[810000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5e3dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final dataframe\n",
    "main = pd.DataFrame(df_main, columns=['USER', 'SENTIMENT', 'ORIGINAL', 'TEXT'])\n",
    "train = pd.DataFrame(df_train, columns=['USER', 'SENTIMENT', 'ORIGINAL', 'TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddbcb6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10000\n",
       "1    10000\n",
       "Name: SENTIMENT, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.SENTIMENT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e03e964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    790000\n",
       "1    790000\n",
       "Name: SENTIMENT, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.SENTIMENT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "439fd200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving final dataframes\n",
    "main.to_csv('../db/04-main-data.csv', index=None)\n",
    "train.to_csv('../db/04-train-data.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5773ec8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1580000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f143d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Major Project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
