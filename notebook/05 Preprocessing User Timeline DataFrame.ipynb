{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944b6254",
   "metadata": {},
   "source": [
    "# File 05: Preprocessing User Timeline DataFrame\n",
    "\n",
    "This file does exactly what you think it does. Preprocessing and a lot of it. Firstly we need to make sure the tweets we feed into the model to run prediction on are in the correct format. We also decided to put a limit on the number of tweets a user should have. Here we are only considering users which have tweets in the range of 100 to 200 as it should give us more accuracy while predicting its accuracy. We also remove any tweets which have less than 3 words in it. \n",
    "\n",
    "### Input Files:\n",
    "- 03-user-tweets-english-only.csv\n",
    "\n",
    "### Output Files:\n",
    "- 05-shortlisted-tweets.csv\n",
    "- 05-shortlisted-usernames.csv\n",
    "\n",
    "### Steps:\n",
    "1. loading required libraries\n",
    "1. read user timeline tweets from dataframe\n",
    "1. create functions that will preprocess the dataset\n",
    "1. preprocessing timeline tweets\n",
    "1. making a list of all usernames\n",
    "1. counting tweets by each user\n",
    "1. shortlisting users with tweet count between 100 and 200\n",
    "1. making final list of tweets and users\n",
    "1. creating dataframes\n",
    "1. saving dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61e9220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Artemis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# loading required python libraries...\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b131325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read user timeline tweets from dataframe\n",
    "df = pd.read_csv('../../db/03-user-tweets-english-only.csv' )\n",
    "user = df.USER.values.tolist()\n",
    "tweet = df.TWEET.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8fd5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Karoli', 'Karoli', 'Karoli']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6f6aef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The FEC is toothless, and Republicans like it that way. No disclosure, no enforcement. But Smith is going full speed ahead anyway.',\n",
       " 'The most cynical statement I have heard in years is coming from Smith, who is calling everyone cynical.',\n",
       " 'I’m listening to the hearing on S1. Bradley Smith frames it as “For the Politicians”. \\n\\nAs if taxpayer money doesn’… https://t.co/yIMX6zqkFe']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "433f466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create functions that will preprocess the dataset\n",
    "porter = PorterStemmer()\n",
    "sw = stopwords.words('english')\n",
    "sw.remove('not')\n",
    "\n",
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def remove_single_chars(text) :\n",
    "    array = text.split()\n",
    "    return (\" \".join([w for w in array if len(w) > 1]))\n",
    "\n",
    "def remove_stopwords(text) :\n",
    "    text = \" \".join([word for word in text.split() if word not in sw])\n",
    "    return text\n",
    "\n",
    "def apply_stemming(text) :\n",
    "    arr1 = text.split(\" \")\n",
    "    arr2 = []\n",
    "    for word in arr1 :\n",
    "        arr2.append(porter.stem(word))\n",
    "    text = \" \".join(arr2)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(sen) :\n",
    "    sentence = remove_tags(sen)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub('@[A-Za-z]+[A-Za-z0-9-_]+', '', sentence)\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    sentence = remove_single_chars(sentence)\n",
    "    sentence = apply_stemming(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b11050fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 953264/953264 [02:18<00:00, 6891.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing timeline tweets...\n",
    "user = df.USER.values.tolist()\n",
    "tweet = df.TWEET.values.tolist()\n",
    "processed = []\n",
    "counts = []\n",
    "\n",
    "for index in tqdm(range(len(df))) :\n",
    "    text = preprocess_text(tweet[index])\n",
    "    processed.append(text)\n",
    "    counts.append(len(text.split()))\n",
    "\n",
    "\n",
    "temp_df = pd.DataFrame(\n",
    "    list(zip(user, tweet, processed)),\n",
    "    columns = ['USER', 'ORIGINAL', 'PROCESSED']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52b510b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 953264/953264 [00:38<00:00, 24917.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# making a list of all usernames\n",
    "username = []\n",
    "for x in tqdm(range(len(user))):\n",
    "    if user[x] not in username :\n",
    "        username.append(user[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f9b9b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9303"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f167f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 9303/9303 [00:54<00:00, 171.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# counting tweets by each user\n",
    "np_user = np.array(user)\n",
    "tweetcount = []\n",
    "for searchval in tqdm(username) :\n",
    "    lst = list(np.where(np_user == searchval)[0])\n",
    "    tweetcount.append(len(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "054bba27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(max(tweetcount))\n",
    "print(min(tweetcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee635b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 9303/9303 [00:00<00:00, 2325364.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# shortlisting users with tweet count between 100 and 200\n",
    "shortlist = []\n",
    "for x in tqdm(range(len(username))) :\n",
    "    if (tweetcount[x] >= 100) and (tweetcount[x] <= 200) :\n",
    "        shortlist.append(username[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3af5444b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5370"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shortlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0293d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 953264/953264 [00:25<00:00, 37030.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# making final list of tweets and users\n",
    "final_user = []\n",
    "final_tweet = []\n",
    "final_original = []\n",
    "for x in tqdm(range(len(user))) :\n",
    "    if user[x] in shortlist :\n",
    "        final_user.append(user[x])\n",
    "        final_tweet.append(processed[x])\n",
    "        final_original.append(tweet[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fcfdcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821719"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2411fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframes\n",
    "final = pd.DataFrame(list(zip(final_user, final_tweet, final_original)), columns=['USER', 'TWEET', 'ORIGINAL'])\n",
    "username = pd.DataFrame(shortlist, columns=['USER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1baab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dataframes\n",
    "final.to_csv('db/05-shortlisted-tweets.csv', index=False)\n",
    "username.to_csv('db/05-shortlisted-usernames.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d15ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Major Project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
