{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559021ff",
   "metadata": {},
   "source": [
    "# File 04: Preprocessing and Splitting Original Dataset\n",
    "\n",
    "This file is quiet essential. This makes sure we have the necessary data to train our model. Our source that set contains 1.6 million labelled tweets. Out of which we have removed 20,000 tweets in 'File 01'. So to avoid training the model on data we might predict on in the future steps, we divide the entire dataset into 'main-data' and 'train-data'.\n",
    "\n",
    "We also preprocess all the tweets in this dataset.\n",
    "The preprocessing involves:\n",
    "- Removing @_usernames_\n",
    "- Removing Hashtags\n",
    "- Removing Hyperlinks\n",
    "- Removing extra spaces\n",
    "- Removing Any digits\n",
    "- Removing Stopwords\n",
    "- Removing Single Characters\n",
    "\n",
    "### Input File:\n",
    "- 1600k-noemoticons.csv\n",
    "\n",
    "### Ouptut File:\n",
    "- 04-main-data.csv -----> 20,000 Entries (10,000 Pos, 10,000 Neg)\n",
    "- 04-train-data.csv ----> 1,580,000 Entries (740,000 Pos, 740,000 Neg)\n",
    "\n",
    "### Steps:\n",
    "1. load required libraries (standard and machine leanring)\n",
    "2. load and format the dataset\n",
    "3. create functions that will preprocess the dataset\n",
    "4. apply preprocessing on all tweets\n",
    "5. convert the results into a dataframe\n",
    "6. split the dataframe into 'main' and 'train'\n",
    "7. save 'main' and 'train' dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63784ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all standard libraries\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c87a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# loading all machine learning libraries\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfaaf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading '1600k-noemoticon.csv' dataset\n",
    "\n",
    "df = pd.read_csv(\"db/1600k-noemoticon.csv\", header=None)\n",
    "df.isnull().values.any()\n",
    "df.rename(\n",
    "    columns = {\n",
    "    0: 'SENTIMENT',\n",
    "    1: 'ID',\n",
    "    2: 'DATE',\n",
    "    3: 'QUERY',\n",
    "    4: 'USERNAME',\n",
    "    5: 'TWEET'\n",
    "    }, inplace=True, errors='raise'\n",
    ")\n",
    "df = df[['USERNAME', 'SENTIMENT', 'TWEET']]\n",
    "df['TEXT'] = \"\"\n",
    "dataset = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d7d0fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TWEET'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45e6c75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# define functions preprocess the dataset...\n"
     ]
    }
   ],
   "source": [
    "# create functions that will preprocess the dataset\n",
    "\n",
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "stopwords = set([\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"])\n",
    "\n",
    "def remove_single_chars(text) :\n",
    "    array = text.split()\n",
    "    return (\" \".join([w for w in array if len(w) > 1]))\n",
    "\n",
    "def remove_stopwords(text) :\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "def preprocess_text(sen) :\n",
    "    sentence = remove_tags(sen)\n",
    "    sentence = re.sub('@[A-Za-z]+[A-Za-z0-9-_]+', '', sentence)\n",
    "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = re.sub('/\\b\\S\\s\\b/', \"\", sentence)\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    sentence = remove_stopwords(sentence)\n",
    "    sentence = remove_single_chars(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d9fb137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1600000/1600000 [00:29<00:00, 53488.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply preprocessing on all tweets\n",
    "\n",
    "for node in tqdm(dataset):\n",
    "    if node[1] > 1 :\n",
    "        node[1] = 1\n",
    "    node[3] = preprocess_text(node[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3457408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "df = pd.DataFrame(dataset, columns=['USER', 'SENTIMENT', 'TWEET', 'TEXT'])\n",
    "df = df[['USER', 'TEXT', 'SENTIMENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a24bfe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe\n",
    "\n",
    "df_list = df.values.tolist()\n",
    "df_main = df_list[:10000] + df_list[800000:810000]\n",
    "df_train = df_list[10000:800000] + df_list[810000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5e3dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final dataframe\n",
    "\n",
    "main = pd.DataFrame(df_main, columns=['USER', 'TEXT', 'SENTIMENT'])\n",
    "train = pd.DataFrame(df_train, columns=['USER', 'TEXT', 'SENTIMENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddbcb6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10000\n",
       "1    10000\n",
       "Name: SENTIMENT, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.SENTIMENT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e03e964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    790000\n",
       "1    790000\n",
       "Name: SENTIMENT, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.SENTIMENT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "439fd200",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.to_csv('db/04-main-data.csv', index=None)\n",
    "train.to_csv('db/04-train-data.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5773ec8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1580000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f143d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_kernel",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
